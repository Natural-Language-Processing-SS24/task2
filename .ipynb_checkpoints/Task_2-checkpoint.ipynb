{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-SS24/task2/blob/main/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB 435.7 kB/s eta 0:00:16\n",
      "    --------------------------------------- 0.1/6.8 MB 1.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/6.8 MB 1.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.4/6.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/6.8 MB 2.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.7/6.8 MB 2.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.8/6.8 MB 2.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/6.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.2/6.8 MB 2.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.3/6.8 MB 2.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.4/6.8 MB 2.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.6/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.7/6.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.2/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.3/6.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.5/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.7/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.8/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.0/6.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.1/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.3/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.5/6.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.7/6.8 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.2/6.8 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.8 MB 3.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.8 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.2/6.8 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.4/6.8 MB 3.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.9/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.3/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.6/6.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/6.8 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 3.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n",
      "Requirement already satisfied: transformers in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: gradio in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: fastapi in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.110.2)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.16.2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.16.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (3.8.0)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (3.10.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (10.0.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (2.7.1)\n",
      "Requirement already satisfied: pydub in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: ruff>=0.2.2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.4.2)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio-client==0.16.2->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from gradio-client==0.16.2->gradio) (10.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.24.1->gradio) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.24.1->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\papad\\anaconda3\\envs\\datascience\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install pyspellchecker\n",
    "!pip install transformers gradio\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8gXRlxho743"
   },
   "source": [
    "## Umgebungseinstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Google Colab spezifische Importe\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "# Datenverarbeitung und Modelltraining\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformer Modelle und Tokenizer\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    GPT2Tokenizer, GPT2ForSequenceClassification,\n",
    "    BartTokenizer, BartForSequenceClassification,\n",
    "    T5Tokenizer, T5ForConditionalGeneration, AdamW,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# PyTorch Bibliotheken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Weitere Bibliotheken\n",
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "import docx\n",
    "from collections import Counter\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten hochladen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Hochladen von Dateien in Google Colab\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    return uploaded\n",
    "\n",
    "# CSV-Datei laden\n",
    "uploaded = upload_files()\n",
    "train_data = pd.read_csv('Sentiment_Training.csv', delimiter=';')\n",
    "test_data = pd.read_csv('Sentiment_Test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der ersten Zeilen und Informationen\n",
    "print(\"Erste Zeilen des Trainingsdatensatzes:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nInformationen zum Trainingsdatensatz:\")\n",
    "print(train_data.info())\n",
    "\n",
    "# Textlänge berechnen\n",
    "train_data['text_length'] = train_data['text'].apply(len)\n",
    "print(\"\\nStatistik der Textlängen im Trainingsdatensatz:\")\n",
    "print(train_data['text_length'].describe())\n",
    "\n",
    "# Histogramm der Textlängen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_data['text_length'], bins=50, edgecolor='black')\n",
    "plt.title('Verteilung der Textlängen')\n",
    "plt.xlabel('Textlänge')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Verteilung der Labels\n",
    "print(\"\\nVerteilung der Labels im Trainingsdatensatz:\")\n",
    "print(train_data['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slang-Wörterbuch laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Lesen der Word-Datei und Erstellen eines Slang-Wörterbuchs\n",
    "def read_slang_dict_from_docx(docx_file):\n",
    "    doc = docx.Document(docx_file)\n",
    "    slang_dict = {}\n",
    "    for para in doc.paragraphs:\n",
    "        if ':' in para.text:\n",
    "            key, value = para.text.split(':', 1)\n",
    "            slang_dict[key.strip().lower()] = value.strip().lower()\n",
    "    return slang_dict\n",
    "\n",
    "# Word-Datei hochladen und lesen\n",
    "uploaded = upload_files()\n",
    "docx_file = 'abbreviations.docx'\n",
    "slang_dict = read_slang_dict_from_docx(docx_file)\n",
    "print(\"Slang Dictionary:\", slang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_slang_words(df, slang_dict):\n",
    "    slang_words_found = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word.lower() in slang_dict:\n",
    "                slang_words_found.append(word.lower())\n",
    "\n",
    "    return Counter(slang_words_found)\n",
    "\n",
    "# Finden von Slang-Wörtern im Trainingsdatensatz\n",
    "slang_words_counter = find_slang_words(train_data, slang_dict)\n",
    "\n",
    "# Anzeige der Slang-Wörter und ihrer Häufigkeit\n",
    "print(slang_words_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textvorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing-Funktion\n",
    "def preprocess_text(text, slang_dict):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    new_words = [slang_dict.get(word.lower(), word) for word in words]\n",
    "    text = ' '.join(new_words)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_parallel(text):\n",
    "    return preprocess_text(text, slang_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Kategorisierung der Sternebewertungen\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 1:\n",
    "        return 'negative'\n",
    "    elif rating == 2:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Anwenden der Funktion auf die Trainings- und Testdaten\n",
    "train_data['sentiment'] = train_data['label'].apply(categorize_rating)\n",
    "test_data['sentiment'] = test_data['label'].apply(categorize_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsdaten in Trainings- und Validierungsdatensätze aufteilen\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, stratify=train_data['sentiment'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset und Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-Klasse definieren\n",
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Funktion zum Tokenisieren der Daten\n",
    "def tokenize_data(data, tokenizer, slang_dict, model_name=None, max_length=128):\n",
    "    with Pool() as pool:\n",
    "        texts = pool.map(preprocess_text_parallel, data['text'].tolist())\n",
    "    labels = pd.Categorical(data['sentiment']).codes\n",
    "    \n",
    "    if model_name == 'T5':\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        targets = [\"positive\" if label == 2 else \"neutral\" if label == 1 else \"negative\" for label in labels]\n",
    "        target_inputs = tokenizer(targets, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        return inputs, target_inputs.input_ids\n",
    "    else:\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer und Modelle definieren\n",
    "model_configs = {\n",
    "    'BERT': {\n",
    "        'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'model': BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "    },\n",
    "    'RoBERTa': {\n",
    "        'tokenizer': RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "        'model': RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "    },\n",
    "    'DistilBERT': {\n",
    "        'tokenizer': DistilBertTokenizer.from_pretrained('distilbert-base-uncased'),\n",
    "        'model': DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "    },\n",
    "    'GPT-2': {\n",
    "        'tokenizer': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "        'model': GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=3)\n",
    "    },\n",
    "    'BART': {\n",
    "        'tokenizer': BartTokenizer.from_pretrained('facebook/bart-large'),\n",
    "        'model': BartForSequenceClassification.from_pretrained('facebook/bart-large', num_labels=3)\n",
    "    },\n",
    "    'T5': {\n",
    "        'tokenizer': T5Tokenizer.from_pretrained('t5-base'),\n",
    "        'model': T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Sicherstellen, dass alle Tokenizer einen pad_token haben\n",
    "for config in model_configs.values():\n",
    "    if config['tokenizer'].pad_token is None:\n",
    "        config['tokenizer'].add_special_tokens({'pad_token': '[PAD]'})\n",
    "        config['model'].resize_token_embeddings(len(config['tokenizer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer definieren\n",
    "def get_optimizer(model):\n",
    "    return AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Trainingsschleife mit Gewichtung\n",
    "def train(model, train_loader, optimizer, device, class_weights, model_name=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    class_weights = class_weights.to(device)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        if model_name == 'T5':\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            weighted_loss = loss * class_weights[labels]\n",
    "            weighted_loss = weighted_loss.mean()\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += weighted_loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluierungsschleife\n",
    "def evaluate(model, val_loader, device, tokenizer, model_name=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            if model_name == 'T5':\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "                preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                true_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "                loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "                total_loss += loss.item()\n",
    "                correct_predictions += sum([1 if pred.strip() == true_label.strip() else 0 for pred, true_label in zip(preds, true_labels)])\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct_predictions += torch.sum(preds == labels).item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "    accuracy = correct_predictions / len(val_loader.dataset)\n",
    "    return total_loss / len(val_loader), accuracy, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle trainieren und speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassen-Gewichtungen\n",
    "class_weights = torch.tensor([1.0, 2.0, 1.0])\n",
    "\n",
    "trained_models = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    tokenizer = config['tokenizer']\n",
    "    model = config['model']\n",
    "\n",
    "    # Tokenisieren der Trainings-, Validierungs- und Testdaten\n",
    "    train_inputs, train_labels = tokenize_data(train_data, tokenizer, slang_dict, model_name)\n",
    "    val_inputs, val_labels = tokenize_data(val_data, tokenizer, slang_dict, model_name)\n",
    "    test_inputs, test_labels = tokenize_data(test_data, tokenizer, slang_dict, model_name)\n",
    "\n",
    "    # Daten in Dataset-Objekte umwandeln\n",
    "    train_dataset = YelpDataset(train_inputs, train_labels)\n",
    "    val_dataset = YelpDataset(val_inputs, val_labels)\n",
    "    test_dataset = YelpDataset(test_inputs, test_labels)\n",
    "\n",
    "    # DataLoader erstellen\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    # Modell auf GPU/CPU laden\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer definieren\n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # Training und Evaluierung\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training {model_name} - Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train(model, train_loader, optimizer, device, class_weights, model_name)\n",
    "        val_loss, val_accuracy, val_labels, val_preds = evaluate(model, val_loader, device, tokenizer, model_name)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Berechnung der Evaluationsmetriken\n",
    "    conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "    class_report = classification_report(val_labels, val_preds, target_names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "    print(f\"\\nConfusion Matrix for {model_name}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # Confusion Matrix plotten\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Modell speichern\n",
    "    trained_models[model_name] = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'class_report': class_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio-Oberfläche erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio-Oberfläche erstellen\n",
    "def classify_text(text):\n",
    "    results = {}\n",
    "    for model_name, model_info in trained_models.items():\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "        if model_name == 'T5':\n",
    "            outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "            sentiments = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "            prediction = sentiments[prediction]\n",
    "        \n",
    "        results[model_name] = prediction\n",
    "\n",
    "    return results\n",
    "\n",
    "# Satz auswählen und Ergebnisse anzeigen\n",
    "def get_test_sentence(index):\n",
    "    return test_data.iloc[index]['text'], test_data.iloc[index]['sentiment']\n",
    "\n",
    "def get_results_for_test_sentence(index):\n",
    "    text, original_label = get_test_sentence(index)\n",
    "    results = classify_text(text)\n",
    "    results['Original Label'] = original_label\n",
    "    return results\n",
    "\n",
    "# Gradio-Komponente für den Satz-Picker\n",
    "sentence_picker = gr.Dropdown(\n",
    "    choices=[f\"{i}: {text}\" for i, text in enumerate(test_data['text'])],\n",
    "    label=\"Wähle einen Satz aus dem Testdatensatz\",\n",
    "    interactive=True\n",
    ")\n",
    "\n",
    "# Gradio-Oberfläche erstellen\n",
    "interface = gr.Interface(\n",
    "    fn=get_results_for_test_sentence,\n",
    "    inputs=sentence_picker,\n",
    "    outputs=\"json\",\n",
    "    title=\"Ergebnisse für einen Satz aus dem Testdatensatz\"\n",
    ")\n",
    "\n",
    "# Starten der Gradio-Oberfläche\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeOJ/jVAbpb0fK0aUttccl",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
