{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Natural-Language-Processing-SS24/task2/blob/main/Rezepte_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment-Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[torch]\n",
    "!pip install pyspellchecker\n",
    "!pip install transformers gradio\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8gXRlxho743"
   },
   "source": [
    "## Umgebungseinstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab spezifische Importe\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "\n",
    "# Datenverarbeitung und Modelltraining\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Transformer Modelle und Tokenizer\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    GPT2Tokenizer, GPT2ForSequenceClassification,\n",
    "    BartTokenizer, BartForSequenceClassification,\n",
    "    T5Tokenizer, T5ForConditionalGeneration, AdamW,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# PyTorch Bibliotheken\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Weitere Bibliotheken\n",
    "from tqdm import tqdm\n",
    "import gradio as gr\n",
    "import docx\n",
    "from collections import Counter\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten hochladen und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Hochladen von Dateien in Google Colab\n",
    "def upload_files():\n",
    "    uploaded = files.upload()\n",
    "    return uploaded\n",
    "\n",
    "# CSV-Datei laden\n",
    "uploaded = upload_files()\n",
    "train_data = pd.read_csv('Sentiment_Training.csv', delimiter=';')\n",
    "test_data = pd.read_csv('Sentiment_Test.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der ersten Zeilen und Informationen\n",
    "print(\"Erste Zeilen des Trainingsdatensatzes:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nInformationen zum Trainingsdatensatz:\")\n",
    "print(train_data.info())\n",
    "\n",
    "# Textlänge berechnen\n",
    "train_data['text_length'] = train_data['text'].apply(len)\n",
    "print(\"\\nStatistik der Textlängen im Trainingsdatensatz:\")\n",
    "print(train_data['text_length'].describe())\n",
    "\n",
    "# Histogramm der Textlängen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train_data['text_length'], bins=50, edgecolor='black')\n",
    "plt.title('Verteilung der Textlängen')\n",
    "plt.xlabel('Textlänge')\n",
    "plt.ylabel('Häufigkeit')\n",
    "plt.show()\n",
    "\n",
    "# Verteilung der Labels\n",
    "print(\"\\nVerteilung der Labels im Trainingsdatensatz:\")\n",
    "print(train_data['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slang-Wörterbuch laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Lesen der Word-Datei und Erstellen eines Slang-Wörterbuchs\n",
    "def read_slang_dict_from_docx(docx_file):\n",
    "    doc = docx.Document(docx_file)\n",
    "    slang_dict = {}\n",
    "    for para in doc.paragraphs:\n",
    "        if ':' in para.text:\n",
    "            key, value = para.text.split(':', 1)\n",
    "            slang_dict[key.strip().lower()] = value.strip().lower()\n",
    "    return slang_dict\n",
    "\n",
    "# Word-Datei hochladen und lesen\n",
    "uploaded = upload_files()\n",
    "docx_file = 'abbreviations.docx'\n",
    "slang_dict = read_slang_dict_from_docx(docx_file)\n",
    "print(\"Slang Dictionary:\", slang_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_slang_words(df, slang_dict):\n",
    "    slang_words_found = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            if word.lower() in slang_dict:\n",
    "                slang_words_found.append(word.lower())\n",
    "\n",
    "    return Counter(slang_words_found)\n",
    "\n",
    "# Finden von Slang-Wörtern im Trainingsdatensatz\n",
    "slang_words_counter = find_slang_words(train_data, slang_dict)\n",
    "\n",
    "# Anzeige der Slang-Wörter und ihrer Häufigkeit\n",
    "print(slang_words_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textvorverarbeitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing-Funktion\n",
    "def preprocess_text(text, slang_dict):\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    new_words = [slang_dict.get(word.lower(), word) for word in words]\n",
    "    text = ' '.join(new_words)\n",
    "    return text\n",
    "\n",
    "def preprocess_text_parallel(text):\n",
    "    return preprocess_text(text, slang_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenvorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Kategorisierung der Sternebewertungen\n",
    "def categorize_rating(rating):\n",
    "    if rating <= 1:\n",
    "        return 'negative'\n",
    "    elif rating == 2:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "\n",
    "# Anwenden der Funktion auf die Trainings- und Testdaten\n",
    "train_data['sentiment'] = train_data['label'].apply(categorize_rating)\n",
    "test_data['sentiment'] = test_data['label'].apply(categorize_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsdaten in Trainings- und Validierungsdatensätze aufteilen\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, stratify=train_data['sentiment'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset und Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-Klasse definieren\n",
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Funktion zum Tokenisieren der Daten\n",
    "def tokenize_data(data, tokenizer, slang_dict, model_name=None, max_length=128):\n",
    "    with Pool() as pool:\n",
    "        texts = pool.map(preprocess_text_parallel, data['text'].tolist())\n",
    "    labels = pd.Categorical(data['sentiment']).codes\n",
    "    \n",
    "    if model_name == 'T5':\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        targets = [\"positive\" if label == 2 else \"neutral\" if label == 1 else \"negative\" for label in labels]\n",
    "        target_inputs = tokenizer(targets, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        return inputs, target_inputs.input_ids\n",
    "    else:\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer und Modelle definieren\n",
    "model_configs = {\n",
    "    'BERT': {\n",
    "        'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'model': BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "    },\n",
    "    'RoBERTa': {\n",
    "        'tokenizer': RobertaTokenizer.from_pretrained('roberta-base'),\n",
    "        'model': RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "    },\n",
    "    'DistilBERT': {\n",
    "        'tokenizer': DistilBertTokenizer.from_pretrained('distilbert-base-uncased'),\n",
    "        'model': DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "    },\n",
    "    'GPT-2': {\n",
    "        'tokenizer': GPT2Tokenizer.from_pretrained('gpt2'),\n",
    "        'model': GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=3)\n",
    "    },\n",
    "    'BART': {\n",
    "        'tokenizer': BartTokenizer.from_pretrained('facebook/bart-base'),\n",
    "        'model': BartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=3)\n",
    "    },\n",
    "    'T5': {\n",
    "        'tokenizer': T5Tokenizer.from_pretrained('t5-base'),\n",
    "        'model': T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training und Evaluierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all tokenizers have a pad_token and adjust models\n",
    "for model_name, config in model_configs.items():\n",
    "    tokenizer = config['tokenizer']\n",
    "    model = config['model']\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Explicitly set the pad_token_id for models that require it\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    assert tokenizer.pad_token is not None, f\"Pad token not added for {model_name}\"\n",
    "\n",
    "# Optimizer definieren\n",
    "def get_optimizer(model):\n",
    "    return AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Trainingsschleife mit Gewichtung\n",
    "def train(model, train_loader, optimizer, device, class_weights, model_name=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    class_weights = class_weights.to(device)\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        if model_name == 'T5':\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            weighted_loss = loss * class_weights[labels]\n",
    "            weighted_loss = weighted_loss.mean()\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += weighted_loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluierungsschleife\n",
    "def evaluate(model, val_loader, device, tokenizer, model_name=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            if model_name == 'T5':\n",
    "                outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "                preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                true_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "                loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "                total_loss += loss.item()\n",
    "                correct_predictions += sum([1 if pred.strip() == true_label.strip() else 0 for pred, true_label in zip(preds, true_labels)])\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                correct_predictions += torch.sum(preds == labels).item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "    accuracy = correct_predictions / len(val_loader.dataset)\n",
    "    return total_loss / len(val_loader), accuracy, all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelle trainieren und speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sicherstellen, dass alle Tokenizer einen pad_token haben und Modelle entsprechend anpassen\n",
    "for model_name, config in model_configs.items():\n",
    "    tokenizer = config['tokenizer']\n",
    "    model = config['model']\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    assert tokenizer.pad_token is not None, f\"Pad token not added for {model_name}\"\n",
    "\n",
    "# Klassen-Gewichtungen\n",
    "class_weights = torch.tensor([1.0, 2.0, 1.0])\n",
    "\n",
    "trained_models = {}\n",
    "for model_name, config in model_configs.items():\n",
    "    tokenizer = config['tokenizer']\n",
    "    model = config['model']\n",
    "\n",
    "    # Tokenisieren der Trainings-, Validierungs- und Testdaten\n",
    "    train_inputs, train_labels = tokenize_data(train_data, tokenizer, slang_dict, model_name)\n",
    "    val_inputs, val_labels = tokenize_data(val_data, tokenizer, slang_dict, model_name)\n",
    "    test_inputs, test_labels = tokenize_data(test_data, tokenizer, slang_dict, model_name)\n",
    "\n",
    "    # Daten in Dataset-Objekte umwandeln\n",
    "    train_dataset = YelpDataset(train_inputs, train_labels)\n",
    "    val_dataset = YelpDataset(val_inputs, val_labels)\n",
    "    test_dataset = YelpDataset(test_inputs, test_labels)\n",
    "\n",
    "    # DataLoader erstellen\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    # Modell auf GPU/CPU laden\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer definieren\n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # Training und Evaluierung\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training {model_name} - Epoch {epoch + 1}/{epochs}\")\n",
    "        train_loss = train(model, train_loader, optimizer, device, class_weights, model_name)\n",
    "        val_loss, val_accuracy, val_labels, val_preds = evaluate(model, val_loader, device, tokenizer, model_name)\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Berechnung der Evaluationsmetriken\n",
    "    conf_matrix = confusion_matrix(val_labels, val_preds)\n",
    "    class_report = classification_report(val_labels, val_preds, target_names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "    print(f\"\\nConfusion Matrix for {model_name}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(class_report)\n",
    "\n",
    "    # Confusion Matrix plotten\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Modell speichern\n",
    "    trained_models[model_name] = {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'conf_matrix': conf_matrix,\n",
    "        'class_report': class_report\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio-Oberfläche erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio-Oberfläche erstellen\n",
    "def classify_text(text):\n",
    "    results = {}\n",
    "    for model_name, model_info in trained_models.items():\n",
    "        model = model_info['model']\n",
    "        tokenizer = model_info['tokenizer']\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128).to(device)\n",
    "        if model_name == 'T5':\n",
    "            outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128)\n",
    "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "            prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "            sentiments = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "            prediction = sentiments[prediction]\n",
    "        \n",
    "        results[model_name] = prediction\n",
    "\n",
    "    return results\n",
    "\n",
    "# Satz auswählen und Ergebnisse anzeigen\n",
    "def get_test_sentence(index):\n",
    "    return test_data.iloc[index]['text'], test_data.iloc[index]['sentiment']\n",
    "\n",
    "def get_results_for_test_sentence(index):\n",
    "    text, original_label = get_test_sentence(index)\n",
    "    results = classify_text(text)\n",
    "    results['Original Label'] = original_label\n",
    "    return results\n",
    "\n",
    "# Gradio-Komponente für den Satz-Picker\n",
    "sentence_picker = gr.Dropdown(\n",
    "    choices=[f\"{i}: {text}\" for i, text in enumerate(test_data['text'])],\n",
    "    label=\"Wähle einen Satz aus dem Testdatensatz\",\n",
    "    interactive=True\n",
    ")\n",
    "\n",
    "# Gradio-Oberfläche erstellen\n",
    "interface = gr.Interface(\n",
    "    fn=get_results_for_test_sentence,\n",
    "    inputs=sentence_picker,\n",
    "    outputs=\"json\",\n",
    "    title=\"Ergebnisse für einen Satz aus dem Testdatensatz\"\n",
    ")\n",
    "\n",
    "# Starten der Gradio-Oberfläche\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeOJ/jVAbpb0fK0aUttccl",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
